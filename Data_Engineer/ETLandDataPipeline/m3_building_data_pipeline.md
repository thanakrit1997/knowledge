
Summary & Highlights
---

Congratulations! You have completed this module. At this point, you know:  

-Apache Airflow is scalable, dynamic, extensible, and lean 

-The five main features of Apache Airflow are pure Python, useful UI, integration, easy to use, and open source  

-A common use case is that Apache Airflow defines and organizes machine learning pipeline dependencies 

-Tasks are created with Airflow operators 

-Pipelines are specified as dependencies between tasks 

-Pipeline DAGs defined as code are more maintainable, testable, and collaborative  

-Apache Airflow has a rich UI that simplifies working with data pipelines 

-You can visualize your DAG in graph or tree mode 

-Key components of a DAG definition file include DAG arguments, DAG and task definitions, and the task pipeline 

-The ‘schedule_interval’ parameter specifies how often to re-run your DAG 

-You can save Airflow logs into local file systems and send them to cloud storage, search engines, and log analyzers 

-Airflow recommends sending production deployment logs to be analyzed by Elasticsearch or Splunk 

-With Airflow’s UI, you can view DAGs and task events 

-The three types of Airflow metrics are counters, gauges, and timers 

-Airflow recommends that production deployment metrics be sent to and analyzed by Prometheus via StatsD 
