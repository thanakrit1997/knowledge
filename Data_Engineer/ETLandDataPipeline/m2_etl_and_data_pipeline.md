Linux Commands and Shell Scripting
---

A shell is a powerful user interface for Unix-like operating systems. It can interpret commands and run other programs. It also enables access to files, utilities, and applications, and is an interactive scripting language. Additionally, you can use a shell to automate tasks. Linux shell commands are used for navigating and working with files and directories. You can also use them for file compression and archiving. 

In this lesson, you will learn about how shell scripting can be used to implement an ETL pipeline, and how ETL scripts or tasks can be scheduled. 

If you are not familiar with Linux commands and shell scripting yet, do enjoy the course ‘Hands-on Introduction to Linux Commands and Shell Scripting’ before diving into this lesson (ETL using Shell Scripting). In the Hands-on Introduction to Linux Commands and Shell Scripting, you will learn about:

The characteristics of Linux commands and shell scripting

The different Linux commands and their outputs

How to schedule jobs using crontab 

How to work with filters, pipes, and variables



Summary & Highlights - ETL using Shell Scripts
---

Congratulations! You have completed this module. At this point, you know:  
- ETL pipelines are created with Bash scripts 
- ETL jobs can be run on a schedule using cron

Summary & Highlights - An Introduction to Data Pipelines
---
- Congratulations! You have completed this module. At this point, you know:  

- Data pipelines move data from one place, or form, to another 

- Data flows through pipelines as a series of data packets 

- Latency and throughput are key design considerations for data pipelines 

- Data pipeline processes include scheduling or triggering, monitoring, maintenance, and optimization 

- Parallelization and I/O buffers can help mitigate bottlenecks 

- Batch pipelines extract and operate on batches of data  

- Batch processing applies when accuracy is critical, or the most recent data isn’t required 

- Streaming data pipelines ingest data packets one-by-one in rapid succession 

- Streaming pipelines apply when the most current data is needed 

- Examples of streaming data pipelines use cases, such as social media feeds, fraud detection, and real-time product pricing 

- Modern data pipeline technologies include schema and transformation support, drag-and-drop GUIs, and security features 

- Stream-processing technologies include Apache Kafka, IBM Streams, and SQLStream 
